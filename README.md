# ğŸ›¡ï¸ Proyecto BI â€“ Counterfeit Product Detection

Este proyecto fue desarrollado por un grupo de tres integrantes con el propÃ³sito de aplicar un flujo de **Inteligencia de Negocios** empleando PostgreSQL, Python (pandas, SQLAlchemy) y Jupyter Notebooks.  

La fuente de informaciÃ³n utilizada proviene del dataset https://www.kaggle.com/datasets/aimlveera/counterfeit-product-detection-dataset , el cual fue organizado en **DataFrames**, cada uno enfocado en un Ã¡rea clave para el anÃ¡lisis.


## ğŸ“Š 1. DataFrame de Transacciones (`df_transacciones`)

### DescripciÃ³n
En este DataFrame se registran todas las operaciones realizadas por los clientes. Se constituye como la base para los anÃ¡lisis financieros y de riesgo dentro del proyecto.

| Columna              | DescripciÃ³n                                |
|----------------------|--------------------------------------------|
| transaction_id       | Identificador Ãºnico de la transacciÃ³n      |
| transaction_date     | Fecha de la operaciÃ³n                      |
| customer_id          | Cliente asociado                           |
| quantity, unit_price | Cantidad y precio unitario                 |
| total_amount         | Monto total                                |
| payment_method       | Forma de pago                              |
| shipping_speed       | Velocidad de envÃ­o                         |
| discount_applied     | Indicador de descuento aplicado            |
| refund_requested     | Solicitud de reembolso                     |
| velocity_flag        | Bandera de velocidad (fraude)              |
| geolocation_mismatch | Bandera de geolocalizaciÃ³n                 |

### Filtros aplicados
1. Transacciones de alto valor (top 10% de `total_amount`).  
2. Operaciones con banderas de riesgo (`velocity_flag`, `geolocation_mismatch`, `refund_requested`).  
3. Compras con descuentos superiores al 30%.  


## ğŸ‘¤ 2. DataFrame de Clientes (`df_clientes`)

### DescripciÃ³n
Este conjunto de datos reÃºne mÃ©tricas a nivel de cliente, con el fin de segmentar y analizar diferentes perfiles de usuarios.

| Columna                      | DescripciÃ³n                                 |
|------------------------------|---------------------------------------------|
| customer_id                  | Identificador Ãºnico del cliente             |
| total_pedidos                | NÃºmero de compras realizadas                |
| monto_total                  | Valor total acumulado                       |
| ticket_promedio              | Valor promedio de compra                    |
| tasa_reembolso               | ProporciÃ³n de pedidos con devoluciÃ³n        |
| flags_riesgo                 | NÃºmero de alertas de fraude asociadas       |
| customer_location_mas_comun  | UbicaciÃ³n mÃ¡s frecuente del cliente         |

### Filtros aplicados
1. Clientes VIP â†’ top 10% en `monto_total`.  
2. Clientes riesgosos â†’ `tasa_reembolso >= 30%` o `flags_riesgo > 0`.  
3. Clientes frecuentes â†’ `total_pedidos >= 5`.  


## ğŸšš 3. DataFrame de LogÃ­stica (`df_logistica`)

### DescripciÃ³n
En este DataFrame se estudia el desempeÃ±o logÃ­stico, con especial atenciÃ³n en los tiempos de entrega y el cumplimiento de los SLA.

| Columna             | DescripciÃ³n                              |
|---------------------|------------------------------------------|
| shipping_speed      | Modalidad de envÃ­o                       |
| delivery_time_days  | Tiempo real de entrega (dÃ­as)            |
| shipping_cost       | Costo del envÃ­o                          |
| sla_dias            | SLA asignado segÃºn modalidad             |
| cumple_sla          | Indicador de cumplimiento del SLA        |

### Filtros aplicados
1. EnvÃ­os fuera de SLA (`cumple_sla = FALSE`).  
2. EnvÃ­os de alto costo (top 10% en `shipping_cost`).  
3. EnvÃ­os con demoras considerables (`delivery_time_days >= 10`).  


## âœ… Conclusiones

- **Transacciones** â†’ se identifican operaciones sospechosas por fraude, alto valor o devoluciones frecuentes.  
- **Clientes** â†’ se segmentan en perfiles estratÃ©gicos como VIP, frecuentes y de alto riesgo.  
- **LogÃ­stica** â†’ se evalÃºa la eficiencia de entregas, costos y el cumplimiento de SLA.  

La construcciÃ³n de estos DataFrames, junto con sus respectivos filtros, permite establecer un flujo de **Inteligencia de Negocios** aplicable a la detecciÃ³n de fraude, la gestiÃ³n de clientes clave y la optimizaciÃ³n de procesos logÃ­sticos.
## ğŸš€ TecnologÃ­as utilizadas
- **PostgreSQL + Docker** â†’ base de datos principal.  
- **Python (pandas, SQLAlchemy)** â†’ procesamiento y anÃ¡lisis de datos.  
- **Jupyter / DataSpell** â†’ entorno de notebooks.  
- **Kaggle Dataset** â†’ fuente de datos (CSV).

# ğŸ“˜ ProyectoG1 â€“ Componente PrÃ¡ctico S2

## ğŸ“Œ IntroducciÃ³n
Este proyecto corresponde al **Componente PrÃ¡ctico S2** de la materia **Inteligencia de Negocios**.  
El objetivo principal fue **preparar y transformar datos de diferentes fuentes** para dejarlos listos para anÃ¡lisis, aplicando tÃ©cnicas de **ExploraciÃ³n, Limpieza, TransformaciÃ³n y ExpansiÃ³n de DataFrames**.

En la prÃ¡ctica profesional, los datos rara vez llegan limpios: suelen contener errores, duplicados o formatos inconsistentes.  
Por eso, aplicamos un proceso **ETL (Extract, Transform, Load)**:

- **Extract (ExtracciÃ³n):** obtuvimos datos desde CSV y los cargamos en PostgreSQL.  
- **Transform (TransformaciÃ³n):** limpiamos, normalizamos y derivamos nuevas variables.  
- **Load (Carga):** guardamos las tablas listas en la base `maestria_bi`.  



## ğŸ—‚ï¸ Fuentes de datos
Se trabajÃ³ con tres datasets principales:

1. **`counterfeit_transactions`**  
   - Contiene transacciones con informaciÃ³n de fechas, montos, mÃ©todos de pago y banderas de fraude.  

2. **`customer_df`**  
   - Incluye informaciÃ³n de clientes: correo, telÃ©fono, IP, direcciÃ³n de facturaciÃ³n y comportamiento histÃ³rico.  

3. **`olist_customers_dataset`**  
   - Proporciona identificadores de clientes junto con ubicaciÃ³n geogrÃ¡fica (cÃ³digo postal, ciudad, estado).  



## âš™ï¸ Desarrollo del Taller

### a. ExploraciÃ³n inicial
- Se identificaron columnas, registros y llaves candidatas (`transaction_id`, `customer_id`).  
- Descubrimos que `customer_df` no tenÃ­a `customer_id`, por lo que se generÃ³ un identificador artificial.

ğŸ‘‰ **Utilidad:** reconocer cÃ³mo se relacionan los datasets y detectar problemas de calidad.



### b. Limpieza de datos
- Normalizamos nombres de columnas (`snake_case`).  
- Eliminamos duplicados.  
- Quitamos valores nulos en campos clave.  

ğŸ‘‰ **Utilidad:** garantizar consistencia y evitar errores en anÃ¡lisis posteriores.


### c. Variables de entorno
Se creÃ³ un archivo **`.env`** con las credenciales de conexiÃ³n a PostgreSQL.  

ğŸ‘‰ **Utilidad:** proteger contraseÃ±as y buenas prÃ¡cticas de seguridad.



### d. Transformaciones
Aplicamos transformaciones relevantes:  
- **Transacciones:** derivamos `anio` y `mes` desde `transaction_date`.  
- **Clientes:** creamos `longitud_email` usando funciones `lambda`.  
- **Olist:** normalizamos `customer_city` y `customer_state`.  

ğŸ‘‰ **Utilidad:** enriquecer los datos para anÃ¡lisis temporal, validaciÃ³n y segmentaciÃ³n.



### e. ExpansiÃ³n de DataFrames
Las nuevas variables fueron integradas en los DataFrames originales.  

ğŸ‘‰ **Utilidad:** mantener datasets completos y listos para consultas.



### f. Ãndices numÃ©ricos
Se generaron IDs Ãºnicos y secuenciales:  
- `id_transaccion`  
- `id_cliente`  
- `id_olist`  

ğŸ‘‰ **Utilidad:** asegurar integridad referencial y facilitar cruces en SQL.



## ğŸ“Š VisualizaciÃ³n de resultados

Con los datos ya listos, construimos grÃ¡ficas para extraer informaciÃ³n:

1. **DistribuciÃ³n de montos de transacciones**  
   Permite detectar clientes con montos atÃ­picamente altos o bajos.  

2. **Clientes por estado**  
   Muestra la concentraciÃ³n geogrÃ¡fica de clientes (Ãºtil en segmentaciÃ³n de mercado).  

3. **Transacciones por mes/aÃ±o**  
   Identifica tendencias estacionales y patrones de compra.  



## ğŸš€ Utilidad del proyecto
- IntegraciÃ³n de datos heterogÃ©neos en una sola base relacional.  
- Aseguramiento de la calidad de los datos para anÃ¡lisis confiables.  
- CreaciÃ³n de indicadores clave (fraude, distribuciÃ³n geogrÃ¡fica, tendencias temporales).  
- Base preparada para **dashboards de BI** o **modelos predictivos**.  


## âœ… Conclusiones
- Los datos crudos no son Ãºtiles sin un proceso de limpieza y transformaciÃ³n.  
- La fase **TransformaciÃ³n (T en ETL)** es crucial para dar valor a la informaciÃ³n.  
- Ahora contamos con una base de datos **integrada, normalizada y enriquecida**, lista para anÃ¡lisis estratÃ©gicos en Inteligencia de Negocios.  


## ğŸ‘¥ Autores
- Equipo G1 â€“ MaestrÃ­a en Ciberseguridad 
  * ORDOÃ‘EZ VIVANCO MARIA FERNANDA
  * MUÃ‘OZ SARMIENTO ANDERSON JOEL
  * ALAVA BOLAÃ‘OS JENNY JULIZZA

- Universidad Internacional del Ecuador (UIDE)  
