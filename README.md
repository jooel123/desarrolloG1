# ğŸ›¡ï¸ Proyecto BI â€“ Inteligencia de Negocios

Este proyecto integra **tres fuentes de datos distintas**, cada una cargada en **PostgreSQL** y analizada con **Python (pandas, SQLAlchemy, Jupyter)**.  
El objetivo es construir **DataFrames principales** para exploraciÃ³n, aplicar filtros representativos y generar valor de negocio a partir de los datos.


## ğŸ“‚ 1. Counterfeit Product Detection Dataset

**Fuente:** Kaggle â€“ [Counterfeit Product Detection](https://www.kaggle.com/datasets/aimlveera/counterfeit-product-detection-dataset)  
**Tabla en PostgreSQL:** `counterfeit_transactions`

### DataFrames principales
- **Transacciones (`df_transacciones`)**  
  Variables: `transaction_id`, `transaction_date`, `customer_id`, `quantity`, `unit_price`, `total_amount`, `payment_method`, `shipping_speed`, `discount_applied`, `refund_requested`, `velocity_flag`, `geolocation_mismatch`.  
  - Filtros:
    1. Transacciones de alto valor (top 10%).  
    2. Transacciones con banderas de riesgo.  
    3. Transacciones con descuentos elevados.

- **Clientes (`df_clientes`)**  
  Variables agregadas: `total_pedidos`, `monto_total`, `ticket_promedio`, `tasa_reembolso`, `flags_riesgo`, `customer_location_mas_comun`.  
  - Filtros:
    1. Clientes VIP (top 10% por monto).  
    2. Clientes riesgosos (alto reembolso o banderas de fraude).  
    3. Clientes frecuentes (â‰¥ 5 pedidos).

- **LogÃ­stica (`df_logistica`)**  
  Variables: `shipping_speed`, `delivery_time_days`, `shipping_cost`, `sla_dias`, `cumple_sla`.  
  - Filtros:
    1. EnvÃ­os fuera de SLA.  
    2. EnvÃ­os costosos (top 10%).  
    3. EnvÃ­os lentos (â‰¥ 10 dÃ­as).


## ğŸ“‚ 2. Olist Customers Dataset

**Fuente:** Kaggle â€“ [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)  
**Tabla en PostgreSQL:** `olist_customers`

### DataFrames principales
- **Clientes (`df_clientes`)**  
  Variables: `customer_id`, `customer_unique_id`, `customer_zip_code_prefix`.  
  - Filtros:
    1. Clientes Ãºnicos por `customer_unique_id`.  
    2. Clientes con ZIP > 90000.  
    3. Clientes duplicados en `customer_id`.

- **Ubicaciones (`df_ubicaciones`)**  
  Variables: `customer_city`, `customer_state`.  
  - Filtros:
    1. Clientes de SÃ£o Paulo (`SP`).  
    2. Clientes de las 10 ciudades mÃ¡s frecuentes.  
    3. Clientes fuera de SP y RJ.

- **Identificadores (`df_identificadores`)**  
  Variables: `customer_id`, `customer_unique_id`.  
  - Filtros:
    1. Conteo de IDs Ãºnicos.  
    2. Duplicados en `customer_unique_id`.  
    3. Top 10 IDs mÃ¡s repetidos.



## ğŸ“‚ 3. Customer_DF Dataset

**Fuente:** Dataset acadÃ©mico de fraude en clientes.  
**Tabla en PostgreSQL:** `customer_df`

### DataFrames principales
- **Contacto (`df_contacto`)**  
  Variables: `customerEmail`, `customerPhone`, `customerDevice`, `customerIPAddress`.  
  - Filtros:
    1. Emails con dominio `gmail.com`.  
    2. Dispositivos mÃ³viles.  
    3. IPs duplicadas.

- **Transacciones (`df_transacciones`)**  
  Variables: `No_Transactions`, `No_Orders`, `No_Payments`.  
  - Filtros:
    1. Clientes con > 5 transacciones.  
    2. Clientes con > 3 Ã³rdenes.  
    3. Clientes con 0 pagos.

- **Riesgo (`df_riesgo`)**  
  Variables: `customerBillingAddress`, `No_Transactions`, `Fraud`.  
  - Filtros:
    1. Clientes marcados como fraude.  
    2. Direcciones de facturaciÃ³n duplicadas.  
    3. Clientes con fraude y mÃ¡s de 2 transacciones.



## âœ… Conclusiones Generales

- **Counterfeit** â†’ Permite detectar fraude en transacciones y optimizar logÃ­stica.  
- **Olist** â†’ Centrado en datos de clientes, Ãºtil para segmentaciÃ³n y geografÃ­a.  
- **Customer_DF** â†’ Enfocado en fraude por contacto/dispositivo, muy Ãºtil en ciberseguridad.  

Este ecosistema de bases de datos brinda un **pipeline completo de BI**, cubriendo **transacciones, clientes y riesgo** desde mÃºltiples fuentes.



## ğŸš€ TecnologÃ­as utilizadas
- **PostgreSQL + Docker** â†’ Almacenamiento relacional.  
- **Python (pandas, SQLAlchemy)** â†’ Procesamiento de datos.  
- **Jupyter / DataSpell** â†’ ExploraciÃ³n y documentaciÃ³n.  
- **Kaggle + datasets acadÃ©micos** â†’ Fuentes de datos abiertas.


### Taller No.2 

# ğŸ“˜ ProyectoG1 â€“ Componente PrÃ¡ctico S2

## ğŸ“Œ IntroducciÃ³n
Este proyecto corresponde al **Componente PrÃ¡ctico S2** de la materia **Inteligencia de Negocios**.  
El objetivo principal fue **preparar y transformar datos de diferentes fuentes** para dejarlos listos para anÃ¡lisis, aplicando tÃ©cnicas de **ExploraciÃ³n, Limpieza, TransformaciÃ³n y ExpansiÃ³n de DataFrames**.

En la prÃ¡ctica profesional, los datos rara vez llegan limpios: suelen contener errores, duplicados o formatos inconsistentes.  
Por eso, aplicamos un proceso **ETL (Extract, Transform, Load)**:

- **Extract (ExtracciÃ³n):** obtuvimos datos desde CSV y los cargamos en PostgreSQL.  
- **Transform (TransformaciÃ³n):** limpiamos, normalizamos y derivamos nuevas variables.  
- **Load (Carga):** guardamos las tablas listas en la base `maestria_bi`.  



## ğŸ—‚ï¸ Fuentes de datos
Se trabajÃ³ con tres datasets principales:

1. **`counterfeit_transactions`**  
   - Contiene transacciones con informaciÃ³n de fechas, montos, mÃ©todos de pago y banderas de fraude.  

2. **`customer_df`**  
   - Incluye informaciÃ³n de clientes: correo, telÃ©fono, IP, direcciÃ³n de facturaciÃ³n y comportamiento histÃ³rico.  

3. **`olist_customers_dataset`**  
   - Proporciona identificadores de clientes junto con ubicaciÃ³n geogrÃ¡fica (cÃ³digo postal, ciudad, estado).  



## âš™ï¸ Desarrollo del Taller

### a. ExploraciÃ³n inicial
- Se identificaron columnas, registros y llaves candidatas (`transaction_id`, `customer_id`).  
- Descubrimos que `customer_df` no tenÃ­a `customer_id`, por lo que se generÃ³ un identificador artificial.

ğŸ‘‰ **Utilidad:** reconocer cÃ³mo se relacionan los datasets y detectar problemas de calidad.


### b. Limpieza de datos
- Normalizamos nombres de columnas (`snake_case`).  
- Eliminamos duplicados.  
- Quitamos valores nulos en campos clave.  

ğŸ‘‰ **Utilidad:** garantizar consistencia y evitar errores en anÃ¡lisis posteriores.


### c. Variables de entorno
Se creÃ³ un archivo **`.env`** con las credenciales de conexiÃ³n a PostgreSQL.  

ğŸ‘‰ **Utilidad:** proteger contraseÃ±as y buenas prÃ¡cticas de seguridad.


### d. Transformaciones
Aplicamos transformaciones relevantes:  
- **Transacciones:** derivamos `anio` y `mes` desde `transaction_date`.  
- **Clientes:** creamos `longitud_email` usando funciones `lambda`.  
- **Olist:** normalizamos `customer_city` y `customer_state`.  

ğŸ‘‰ **Utilidad:** enriquecer los datos para anÃ¡lisis temporal, validaciÃ³n y segmentaciÃ³n.


### e. ExpansiÃ³n de DataFrames
Las nuevas variables fueron integradas en los DataFrames originales.  

ğŸ‘‰ **Utilidad:** mantener datasets completos y listos para consultas.


### f. Ãndices numÃ©ricos
Se generaron IDs Ãºnicos y secuenciales:  
- `id_transaccion`  
- `id_cliente`  
- `id_olist`  

ğŸ‘‰ **Utilidad:** asegurar integridad referencial y facilitar cruces en SQL.


## ğŸ“Š VisualizaciÃ³n de resultados

Con los datos ya listos, construimos grÃ¡ficas para extraer informaciÃ³n:

1. **DistribuciÃ³n de montos de transacciones**  
   Permite detectar clientes con montos atÃ­picamente altos o bajos.  

2. **Clientes por estado**  
   Muestra la concentraciÃ³n geogrÃ¡fica de clientes (Ãºtil en segmentaciÃ³n de mercado).  

3. **Transacciones por mes/aÃ±o**  
   Identifica tendencias estacionales y patrones de compra.  


## ğŸš€ Utilidad del proyecto
- IntegraciÃ³n de datos heterogÃ©neos en una sola base relacional.  
- Aseguramiento de la calidad de los datos para anÃ¡lisis confiables.  
- CreaciÃ³n de indicadores clave (fraude, distribuciÃ³n geogrÃ¡fica, tendencias temporales).  
- Base preparada para **dashboards de BI** o **modelos predictivos**.  


## âœ… Conclusiones
- Los datos crudos no son Ãºtiles sin un proceso de limpieza y transformaciÃ³n.  
- La fase **TransformaciÃ³n (T en ETL)** es crucial para dar valor a la informaciÃ³n.  
- Ahora contamos con una base de datos **integrada, normalizada y enriquecida**, lista para anÃ¡lisis estratÃ©gicos en Inteligencia de Negocios.  


## ğŸ‘¥ Autores
- Equipo G1 â€“ MaestrÃ­a en Ciberseguridad 
    - ALAVA BOLAÃ‘OS JENNY JULIZZA
    - MUÃ‘OZ SARMIENTO ANDERSON JOEL
    - ORDOÃ‘EZ VIVANCO MARIA FERNANDA

# ğŸ“˜ ProyectoG1 â€“ Componente PrÃ¡ctico S2

## ğŸ“Œ IntroducciÃ³n
Este proyecto corresponde al **Componente PrÃ¡ctico S2** de la materia **Inteligencia de Negocios**.  
El objetivo principal fue **preparar y transformar datos de diferentes fuentes** para dejarlos listos para anÃ¡lisis, aplicando tÃ©cnicas de **ExploraciÃ³n, Limpieza, TransformaciÃ³n y ExpansiÃ³n de DataFrames**.

En la prÃ¡ctica profesional, los datos rara vez llegan limpios: suelen contener errores, duplicados o formatos inconsistentes.  
Por eso, aplicamos un proceso **ETL (Extract, Transform, Load)**:

- **Extract (ExtracciÃ³n):** obtuvimos datos desde CSV y los cargamos en PostgreSQL.  
- **Transform (TransformaciÃ³n):** limpiamos, normalizamos y derivamos nuevas variables.  
- **Load (Carga):** guardamos las tablas listas en la base `maestria_bi`.  


## ğŸ—‚ï¸ Fuentes de datos
Se trabajÃ³ con tres datasets principales:

1. **`counterfeit_transactions`**  
   - Contiene transacciones con informaciÃ³n de fechas, montos, mÃ©todos de pago y banderas de fraude.  

2. **`customer_df`**  
   - Incluye informaciÃ³n de clientes: correo, telÃ©fono, IP, direcciÃ³n de facturaciÃ³n y comportamiento histÃ³rico.  

3. **`olist_customers_dataset`**  
   - Proporciona identificadores de clientes junto con ubicaciÃ³n geogrÃ¡fica (cÃ³digo postal, ciudad, estado).  



## âš™ï¸ Desarrollo del Taller

### a. ExploraciÃ³n inicial
- Se identificaron columnas, registros y llaves candidatas (`transaction_id`, `customer_id`).  
- Descubrimos que `customer_df` no tenÃ­a `customer_id`, por lo que se generÃ³ un identificador artificial.

ğŸ‘‰ **Utilidad:** reconocer cÃ³mo se relacionan los datasets y detectar problemas de calidad.



### b. Limpieza de datos
- Normalizamos nombres de columnas (`snake_case`).  
- Eliminamos duplicados.  
- Quitamos valores nulos en campos clave.  

ğŸ‘‰ **Utilidad:** garantizar consistencia y evitar errores en anÃ¡lisis posteriores.


### c. Variables de entorno
Se creÃ³ un archivo **`.env`** con las credenciales de conexiÃ³n a PostgreSQL.  

ğŸ‘‰ **Utilidad:** proteger contraseÃ±as y buenas prÃ¡cticas de seguridad.



### d. Transformaciones
Aplicamos transformaciones relevantes:  
- **Transacciones:** derivamos `anio` y `mes` desde `transaction_date`.  
- **Clientes:** creamos `longitud_email` usando funciones `lambda`.  
- **Olist:** normalizamos `customer_city` y `customer_state`.  

ğŸ‘‰ **Utilidad:** enriquecer los datos para anÃ¡lisis temporal, validaciÃ³n y segmentaciÃ³n.

### e. ExpansiÃ³n de DataFrames
Las nuevas variables fueron integradas en los DataFrames originales.  

ğŸ‘‰ **Utilidad:** mantener datasets completos y listos para consultas.



### f. Ãndices numÃ©ricos
Se generaron IDs Ãºnicos y secuenciales:  
- `id_transaccion`  
- `id_cliente`  
- `id_olist`  

ğŸ‘‰ **Utilidad:** asegurar integridad referencial y facilitar cruces en SQL.



## ğŸ“Š VisualizaciÃ³n de resultados

Con los datos ya listos, construimos grÃ¡ficas para extraer informaciÃ³n:

1. **DistribuciÃ³n de montos de transacciones**  
   Permite detectar clientes con montos atÃ­picamente altos o bajos.  

2. **Clientes por estado**  
   Muestra la concentraciÃ³n geogrÃ¡fica de clientes (Ãºtil en segmentaciÃ³n de mercado).  

3. **Transacciones por mes/aÃ±o**  
   Identifica tendencias estacionales y patrones de compra.  



## ğŸš€ Utilidad del proyecto
- IntegraciÃ³n de datos heterogÃ©neos en una sola base relacional.  
- Aseguramiento de la calidad de los datos para anÃ¡lisis confiables.  
- CreaciÃ³n de indicadores clave (fraude, distribuciÃ³n geogrÃ¡fica, tendencias temporales).  
- Base preparada para **dashboards de BI** o **modelos predictivos**.  


## âœ… Conclusiones
- Los datos crudos no son Ãºtiles sin un proceso de limpieza y transformaciÃ³n.  
- La fase **TransformaciÃ³n (T en ETL)** es crucial para dar valor a la informaciÃ³n.  
- Ahora contamos con una base de datos **integrada, normalizada y enriquecida**, lista para anÃ¡lisis estratÃ©gicos en Inteligencia de Negocios.  




# ğŸ“Š PrÃ¡ctica 3 â€” IntegraciÃ³n de Datos con Redshift y S3

Este proyecto corresponde a la **PrÃ¡ctica 3 de la MaestrÃ­a en Ciberseguridad y BI (UIDE)**.  
El objetivo fue **crear un Data Warehouse en Amazon Redshift**, cargando datos desde archivos CSV en Amazon S3, para luego realizar consultas analÃ­ticas.

---

## ğŸš€ Pasos Realizados

### 1ï¸âƒ£ PreparaciÃ³n del Entorno
- Se configuraron credenciales de **AWS IAM** con permisos sobre S3 y Redshift.
- Se creÃ³ un **bucket S3** llamado: `intelligents`.
- Se habilitÃ³ un **workgroup Redshift Serverless** (`uide-workgroup`) y un **namespace** (`redshift-uide`).

ğŸ“¸ **Evidencia:**  
- ![CreaciÃ³n bucket S3](images/creacion-bucket-s3.png)  
- ![ConfiguraciÃ³n IAM](images/configuraciondemi-iam.png)  
- ![Redshift creado](images/portal-de-amazon-redshift-ya-creado.png)  

---

### 2ï¸âƒ£ ExportaciÃ³n y Subida de Datos
- Se exportaron los dataframes de la prÃ¡ctica en **archivos CSV**.  
- Archivos generados:
  - `dim_clientes.csv`
  - `dim_envio.csv`
  - `dim_geografia.csv`
  - `dim_pago.csv`
  - `dim_tiempo.csv`
  - `fact_transacciones.csv`
- Los CSV fueron subidos exitosamente a **S3** en la carpeta `data/`.

ğŸ“¸ **Evidencia:**  
- ![Archivos exportados a S3](images/archivos-exportados-albucket-s3.png)  
- ![Subida exitosa a S3](images/archivos-subidos-exitosamente-amazonS3.png)  

---

### 3ï¸âƒ£ CreaciÃ³n de Tablas en Redshift
- Se conectÃ³ Python (PyCharm / Jupyter) con Redshift usando `psycopg2`.  
- Se ejecutaron scripts SQL para crear las tablas dimensionales y de hechos:  
  - `dim_clientes`  
  - `dim_envio`  
  - `dim_geografia`  
  - `dim_pago`  
  - `dim_tiempo`  
  - `fact_transacciones`

ğŸ“¸ **Evidencia:**  
- ![Esquema modelo estrella](images/modelo_estrella_bi.png)  
- ![Base de datos Redshift](images/databases.png)  

---

### 4ï¸âƒ£ Carga de Datos con COPY desde S3
- Se configurÃ³ un **IAM Role** (`RedshiftS3AccessRole`) con permisos de `AmazonS3FullAccess`.
- Se usÃ³ el comando `COPY` para cargar los datos desde los archivos CSV en S3 a las tablas en Redshift.

ğŸ“¸ **Evidencia:**  
- ![Subida CSV a Redshift](images/subida-csv-datos.png)  
- ![VisualizaciÃ³n datos Redshift](images/visualizacion-database-redshift.png)  

---

### 5ï¸âƒ£ ValidaciÃ³n y Consultas
- Se verificÃ³ el conteo de registros por tabla en Redshift:  

```sql
SELECT 'dim_tiempo', COUNT(*) FROM dim_tiempo
UNION ALL
SELECT 'dim_clientes', COUNT(*) FROM dim_clientes
UNION ALL
SELECT 'dim_geografia', COUNT(*) FROM dim_geografia
UNION ALL
SELECT 'dim_pago', COUNT(*) FROM dim_pago
UNION ALL
SELECT 'dim_envio', COUNT(*) FROM dim_envio
UNION ALL
SELECT 'fact_transacciones', COUNT(*) FROM fact_transacciones;

## ğŸ‘¥ Autores
- Equipo G1 â€“ MaestrÃ­a en Ciberseguridad 
  * ORDOÃ‘EZ VIVANCO MARIA FERNANDA
  * MUÃ‘OZ SARMIENTO ANDERSON JOEL
  * ALAVA BOLAÃ‘OS JENNY JULIZZA

- Universidad Internacional del Ecuador (UIDE)  
